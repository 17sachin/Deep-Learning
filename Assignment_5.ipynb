{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYpNoB+YAs9Q+88AE9KS39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Why would you want to use the Data API?\n",
        "\n",
        "APIs are needed to bring applications together in order to perform a designed function built around sharing data and executing pre-defined processes. They work as the middle man, allowing developers to build new programmatic interactions between the various applications people and businesses use on a daily basis."
      ],
      "metadata": {
        "id": "O4R2B-8UfeR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the benefits of splitting a large dataset into multiple files?\n",
        "\n",
        "Splitting a shared database can help improve its performance and reduce the chance of database file corruption. After you split database, you may decide to move the back-end database, or to use a different back-end database. You can use the Linked Table Manager to change the back-end database that you use."
      ],
      "metadata": {
        "id": "xQGyUGezfmXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
        "\n",
        "A bottleneck is a point of congestion in a production system (such as an assembly line or a computer network) that occurs when workloads arrive too quickly for the production process to handle. The inefficiencies brought about by the bottleneck often creates delays and higher production costs.\n",
        "\n"
      ],
      "metadata": {
        "id": "WZVVyaJNfuz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
        "\n",
        "Step 1: Create a Free Roboflow Public Workspace. Roboflow is the universal conversion tool for computer vision annotation formats. \n",
        "Step 2: Upload your data into Roboflow. \n",
        "Step 3: Generate Dataset Version.\n",
        "Step 4: Export Dataset Version.\n"
      ],
      "metadata": {
        "id": "iLAiUa_tgNkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
        "\n"
      ],
      "metadata": {
        "id": "7A0W69xfgfgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
        "\n",
        "The TFRecord format is a simple format for storing a sequence of binary records. Converting your data into TFRecord has many advantages, such as: More efficient storage: the TFRecord data can take up less space than the original data; it can also be partitioned into multiple files.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fm3TlPXvgnh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
        "\n",
        "The tf. data. Dataset API supports writing descriptive and efficient input pipelines.Apply dataset transformations to preprocess the data. Iterate over the dataset and process the elements\n",
        "\n",
        "Importing Data. Create a Dataset instance from some data.\n",
        "Create an Iterator. By using the created dataset to make an Iterator instance to iterate through the dataset.\n",
        "Consuming Data. By using the created iterator we can get the elements from the dataset to feed the model."
      ],
      "metadata": {
        "id": "f8JuBSMHgyCI"
      }
    }
  ]
}