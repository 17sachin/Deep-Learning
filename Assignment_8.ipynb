{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtcZL6x9ZRrvWYjPgTDyLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
        "\n",
        "Setting an RNN to be stateful means that it can build a state across its training sequence and even maintain that state when doing predictions. The benefits of using stateful RNNs are smaller network sizes and/or lower training times.\n",
        "\n",
        "In stateless cases, LSTM updates parameters on batch1 and then, initiate hidden states and cell states (usually all zeros) for batch2, while in stateful cases, it uses batch1's last output hidden states and cell sates as initial states for batch2"
      ],
      "metadata": {
        "id": "YFIkLkLDzaU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
        "\n",
        "This two-step model, called an Encoder–Decoder, works much better than trying to translate on the fly with a single sequence-to-sequence RNN (like the one represented on the top left), since the last words of a sentence can affect the first words of the translation, so you need to wait until you have heard the whole.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPaL-TOizlgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
        "\n",
        "In the case of variable length sequence prediction problems, this requires that your data be transformed such that each sequence has the same length. This vectorization allows code to efficiently perform the matrix operations in batch for your chosen deep learning algorithms.\n",
        "\n",
        "For variable sized inputs where there is no particular ordering among the inputs, one can design networks which: use a repetition of the same subnetwork for each of the groups of inputs (i.e. with shared weights). This repeated subnetwork learns a representation of the (groups of) inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "Az4F7b5OzvwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is beam search and why would you use it? What tool can you use to implement it?\n",
        "\n",
        "A beam search is most often used to maintain tractability in large systems with insufficient amount of memory to store the entire search tree. For example, it has been used in many machine translation systems. (The state of the art now primarily uses neural machine translation based methods.)\n",
        "\n",
        "Beam search is an algorithm used in many NLP and speech recognition models as a final decision making layer to choose the best output given target variables like maximum probability or next output character.\n",
        "\n"
      ],
      "metadata": {
        "id": "SuRNDWji0Eek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is an attention mechanism? How does it help?\n",
        "\n",
        "The Attention mechanism has revolutionised the way we create NLP models and is currently a standard fixture in most state-of-the-art NLP models. This is because it enables the model to “remember” all the words in the input and focus on specific words when formulating a response.\n",
        "\n",
        "Attention is one of the most influential ideas in the Deep Learning community. Even though this mechanism is now used in various problems like image captioning and others,it was initially designed in the context of Neural Machine Translation using Seq2Seq Models"
      ],
      "metadata": {
        "id": "zfiN-A0D0RA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the most important layer in the Transformer architecture? What is its purpose?\n",
        "\n",
        "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the field of natural language processing (NLP) and in computer vision (CV).\n",
        "\n",
        "The most important part here is the “Residual Connections” around the layers. This is very important in retaining the position related information which we are adding to the input representation/embedding across the network."
      ],
      "metadata": {
        "id": "SbMaSQF40at1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When would you need to use sampled softmax?\n",
        "\n",
        "Sampled softmax aims to approximate a full softmax during model training (Bengio & Sénécal, 2008; 2003). Rather than computing the loss over all classes, only the positive class and a sample of m negative classes are considered. Each negative class is sampled with probability qi with replacement."
      ],
      "metadata": {
        "id": "kOmND0fo0j_2"
      }
    }
  ]
}