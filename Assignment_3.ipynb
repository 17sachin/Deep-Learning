{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMU5dGz0U1VxyW2Xqsz1p+Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
        "\n",
        "The network has one hidden and input layer with two neurons, an output layer with four neurons. Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value. Key points. Never initialize all the weights to zero."
      ],
      "metadata": {
        "id": "dgFEHAJPJcpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Is it OK to initialize the bias terms to 0?\n",
        "\n",
        "In general practice biases are initialized with 0 and weights are initialized with random numbers, what if weights are initialized with 0? In order to understand let us consider we applied sigmoid activation function for the output layer"
      ],
      "metadata": {
        "id": "r0pB3dIaJyj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Name three advantages of the SELU activation function over ReLU.\n",
        "\n",
        "ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple."
      ],
      "metadata": {
        "id": "QJG--aekJ6N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "Sigmoid and tanh should be avoided due to vanishing gradient problem.\n",
        "Softplus and Softsign should also be avoided as Relu is a better choice.\n",
        "Relu should be preferred for hidden layers.\n",
        "For deep networks, swish performs better than relu"
      ],
      "metadata": {
        "id": "Fy8CwaG7KG2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
        "\n",
        "The hyperparameter λ controls this tradeoff by adjusting the weight of the penalty term. If λ is increased, model complexity will have a greater contribution to the cost. Because the minimum cost hypothesis is selected, this means that higher λ will bias the selection toward models with lower complexity.\n"
      ],
      "metadata": {
        "id": "QlxVTWFKKS2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Name three ways you can produce a sparse model.\n",
        "\n",
        "Controlled sparsity occurs when a range of values of one or more dimensions has no data; for example, a new variable dimensioned by MONTH for which you do not have data for past months. The cells exist because you have past months in the MONTH dimension, but the data is NA."
      ],
      "metadata": {
        "id": "cvO5LVcXKe81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
        "\n",
        "In the original implementation of dropout, dropout does work in both training time and inference time. During training time, dropout randomly sets node values to zero. In the original implementation, we have “keep probability” pkeep .\n",
        "\n",
        "In theory dropout layer shouldn't impact inference performance. But in the code above adding dropout layer increase single-image prediction time in 1.5 times and 10-images batch prediction is almost twice slower than without dropout."
      ],
      "metadata": {
        "id": "MbfG0C2pKvVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Practice training a deep neural network on the CIFAR10 image dataset:"
      ],
      "metadata": {
        "id": "K5D3ZyfBK8AX"
      }
    }
  ]
}