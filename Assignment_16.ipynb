{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPj1p+4leFtZEysVSJ+pi6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Explain the Activation Functions in your own language\n",
        "a) sigmoid:\n",
        "Sigmoid Function acts as an activation function in machine learning which is used to add non-linearity in a machine learning model, in simple words it decides which value to pass as output and what not to pass, there are mainly 7 types of Activation Functions which are used in machine learning and deep learning\n",
        "\n",
        "b) tanh:\n",
        "Tanh is the hyperbolic tangent function, which is the hyperbolic analogue of the Tan circular function used throughout trigonometry. Tanh[α] is defined as the ratio of the corresponding hyperbolic sine and hyperbolic cosine functions via . The inverse function of Tanh is ArcTanh.\n",
        "\n",
        "c) ReLU:\n",
        "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
        "\n",
        "d) ELU:\n",
        "Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.They are both in identity function form for non-negative inputs\n",
        "\n",
        "e) LeakyReLU:\n",
        "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope.This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.\n",
        "\n",
        "f) swish:\n",
        "to move, pass, swing, or whirl with the sound of a swish. transitive verb. 1 : to move, cut, or strike with a swish the horse swished its tail. 2 : to make (a basketball shot) so that the ball falls through the rim without touching it swished a 3-point jumper.\n",
        "\n"
      ],
      "metadata": {
        "id": "ElgalL8rGm-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What happens when you increase or decrease the optimizer learning rate?\n",
        "\n",
        "The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs."
      ],
      "metadata": {
        "id": "0sQWCbXMHPki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What happens when you increase the number of internal hidden neurons?\n",
        "\n",
        "Increasing the number of hidden layers might improve the accuracy or might not, it really depends on the complexity of the problem that you are trying to solve. Where in the left picture they try to fit a linear function to the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "mYq3l7jFHaD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What happens when you increase the size of batch computation?\n",
        "\n",
        "Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning."
      ],
      "metadata": {
        "id": "33jE8yW9HlD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Why we adopt regularization to avoid overfitting?\n",
        "\n",
        "Regularization comes into play and shrinks the learned estimates towards zero. In other words, it tunes the loss function by adding a penalty term, that prevents excessive fluctuation of the coefficients. Thereby, reducing the chances of overfitting.\n",
        "\n",
        "Regularization, significantly reduces the variance of the model, without substantial increase in its bias.As the value of λ rises, it reduces the value of coefficients and thus reducing the variance."
      ],
      "metadata": {
        "id": "vfpZGhJIHzTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What are loss and cost functions in deep learning?\n",
        "\n",
        "The terms cost and loss functions almost refer to the same meaning. But, loss function mainly applies for a single training set as compared to the cost function which deals with a penalty for a number of training sets or the complete batch. It is also sometimes called an error function.\n"
      ],
      "metadata": {
        "id": "Ss7aWlSAIA3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What do you mean by underfitting in neural networks?\n",
        "\n",
        "Underfitting is on the opposite end of the spectrum. A model is said to be underfitting when it's not even able to classify the data it was trained on, let alone data it hasn't seen before. A model is said to be underfitting when it's not able to classify the data it was trained on"
      ],
      "metadata": {
        "id": "8SLqmqVZIKF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Why we use Dropout in Neural Networks?\n",
        "\n",
        "Dropout can be used after convolutional layers (e.g. Conv2D) and after pooling layers (e.g. MaxPooling2D). Often, dropout is only used after the pooling layers, but this is just a rough heuristic. In this case, dropout is applied to each element or cell within the feature maps."
      ],
      "metadata": {
        "id": "oOrjzm20IVfM"
      }
    }
  ]
}