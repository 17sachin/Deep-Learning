{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1GIX3kgexD69IV496a3JO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What does a SavedModel contain? How do you inspect its content?\n",
        "\n",
        "A SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies.\n",
        "\n",
        "SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel. For example, you can use the CLI to inspect the model's SignatureDef s. The CLI enables you to quickly confirm that the input Tensor dtype and shape match the model."
      ],
      "metadata": {
        "id": "kgdR3czW4lr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
        "\n",
        "TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs.\n",
        "\n",
        "BentoML. BentoML is a framework for serving, managing, and deploying machine learning models. \n",
        "Cortex. \n",
        "TensorFlow Serving. \n",
        "TorchServe. \n",
        "KFServing. \n",
        "Multi Model Server. \n",
        "Triton Inference Server.\n",
        "ForestFlow"
      ],
      "metadata": {
        "id": "oqP_X02Q44fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.How do you deploy a model across multiple TF Serving instances?\n",
        "\n",
        "Step 1: Install the Docker App.\n",
        "\n",
        "Step 2: Pull the TensorFlow Serving Image. docker pull tensorflow/serving. \n",
        "\n",
        "Step 3: Create and Train the Model.\n",
        "\n",
        "Step 4: Save the Model. \n",
        "\n",
        "Step 5: Serving the model using Tensorflow Serving. \n",
        "\n",
        "Step 6: Make a REST request the model to predict."
      ],
      "metadata": {
        "id": "OiaCJRy65cd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
        "\n",
        "REST provides a request-response communication model built on the HTTP 1.1 protocol. However, gRPC follows a client-response model of communication for designing web APIs that rely on HTTP/2. Hence, gRPC allows streaming communication and serves multiple requests simultaneously."
      ],
      "metadata": {
        "id": "hNTls_az5QPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?\n",
        "\n",
        "Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\"\n",
        "\n",
        "Train Keras model to reach an acceptable accuracy as always.\n",
        "\n",
        "Make Keras layers or model ready to be pruned.\n",
        "\n",
        "Create a pruning schedule and train the model for more epochs.\n",
        "\n",
        "Export the pruned model by striping pruning wrappers from the model."
      ],
      "metadata": {
        "id": "N1zvxgGi6Jfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is quantization-aware training, and why would you need it?\n",
        "\n",
        "Quantization aware training emulates inference-time quantization, creating a model that downstream tools will use to produce actually quantized models. The quantized models use lower-precision (e.g. 8-bit instead of 32-bit float), leading to benefits during deployment."
      ],
      "metadata": {
        "id": "ayiOBlN96Xrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What are model parallelism and data parallelism? Why is the latter\n",
        "generally recommended?\n",
        "\n",
        "Data parallelism is when you use the same model for every thread, but feed it with different parts of the data; model parallelism is when you use the same data for every thread, but split the model among threads.\n",
        "\n",
        "Model parallelism is the process of splitting a model up between multiple devices or nodes (such as GPU-equipped instances) and creating an efficient pipeline to train the model across these devices to maximize GPU utilization.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOzbMOd36gXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
        "\n",
        "There are generally two ways to distribute computation across multiple devices: Data parallelism, where a single model gets replicated on multiple devices or multiple machines. Each of them processes different batches of data, then they merge their results.\n",
        "\n"
      ],
      "metadata": {
        "id": "seR_W11N6wP0"
      }
    }
  ]
}