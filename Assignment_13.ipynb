{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCaiCfa/PMdeAGnmpoPQu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
        "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
        "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
        "classifier?\n",
        "\n",
        "The logistic regression classifier is able to converge on non-linear data and outputs class probabilities.\n",
        "\n",
        "Originally a perceptron was only referring to neural networks with a step function as the transfer function. In that case of course the difference is that the logistic regression uses a logistic function and the perceptron uses a step function."
      ],
      "metadata": {
        "id": "iz-sBGP-X0Tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Why was the logistic activation function a key ingredient in training the first MLPs?\n",
        "\n",
        "Because the derivative of the logistic function is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all."
      ],
      "metadata": {
        "id": "sF9j5mC-X8qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Name three popular activation functions. Can you draw them?\n",
        "\n",
        "For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) and predicting a numerical variable (regression). If your problem is a regression problem, you should use a linear activation function. Regression: One node, linear activation."
      ],
      "metadata": {
        "id": "hROcWVWDYF0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is backpropagation and how does it work? What is the difference between\n",
        "backpropagation and reverse-mode autodiff?\n",
        "\n",
        "I think the difference is that back-propagation refers to the updating of weights with respect to their gradient to minimize a function; \"back-propagating the gradients\" is a typical term used. Conversely, reverse-mode diff merely means calculating the gradient of a function."
      ],
      "metadata": {
        "id": "RFbMcGk6YNWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
        "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "\n",
        "The hyperparameters to tune are the number of neurons, activation function, optimizer, learning rate, batch size, and epochs."
      ],
      "metadata": {
        "id": "Z2AoFdSQYVku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HjP-EIvIYcAV"
      }
    }
  ]
}