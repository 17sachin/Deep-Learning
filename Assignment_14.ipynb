{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_14.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6fp/GGo4KHy/9Tbnl7+qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Deep-Learning/blob/main/Assignment_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
        "\n",
        "Initializing all the weights with zeros leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly. ... Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things."
      ],
      "metadata": {
        "id": "o3tJ-wOAE4G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Is it okay to initialize the bias terms to 0?\n",
        "\n",
        "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights."
      ],
      "metadata": {
        "id": "p1Zbs0XkFBcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Name three advantages of the ELU activation function over ReLU.\n",
        "\n",
        "ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple."
      ],
      "metadata": {
        "id": "9JV8G4clFJqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "Sigmoid functions and their combinations generally work better in the case of classifiers.\n",
        "Sigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem.\n",
        "ReLU function is a general activation function and is used in most cases these days."
      ],
      "metadata": {
        "id": "cy8ZijcXFXcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
        "\n",
        "If your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. However, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.\n",
        "\n"
      ],
      "metadata": {
        "id": "WXRnaZS5Fl-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Name three ways you can produce a sparse model.\n",
        "\n",
        "Controlled sparsity occurs when a range of values of one or more dimensions has no data; for example, a new variable dimensioned by MONTH for which you do not have data for past months. The cells exist because you have past months in the MONTH dimension, but the data is NA."
      ],
      "metadata": {
        "id": "uODZrzY0FxHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
        "\n",
        "In the original implementation of dropout, dropout does work in both training time and inference time. During training time, dropout randomly sets node values to zero. In the original implementation, we have “keep probability” pkeep . So dropout randomly kills node values with “dropout probability” 1−pkeep 1 − p keep .\n",
        "\n"
      ],
      "metadata": {
        "id": "DgnI0owtF732"
      }
    }
  ]
}